{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flappy-bird-gymnasium in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: gymnasium in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from flappy-bird-gymnasium) (1.0.0)\n",
      "Requirement already satisfied: numpy in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from flappy-bird-gymnasium) (1.26.4)\n",
      "Requirement already satisfied: pygame in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from flappy-bird-gymnasium) (2.6.1)\n",
      "Requirement already satisfied: matplotlib in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from flappy-bird-gymnasium) (3.10.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from gymnasium->flappy-bird-gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from gymnasium->flappy-bird-gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from gymnasium->flappy-bird-gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from matplotlib->flappy-bird-gymnasium) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from matplotlib->flappy-bird-gymnasium) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from matplotlib->flappy-bird-gymnasium) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from matplotlib->flappy-bird-gymnasium) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from matplotlib->flappy-bird-gymnasium) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from matplotlib->flappy-bird-gymnasium) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from matplotlib->flappy-bird-gymnasium) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from matplotlib->flappy-bird-gymnasium) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->flappy-bird-gymnasium) (1.16.0)\n",
      "Requirement already satisfied: gymnasium in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyyaml\n",
      "Successfully installed pyyaml-6.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install flappy-bird-gymnasium\n",
    "!pip install gymnasium\n",
    "\n",
    "!pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "377.85s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "384.10s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/swl/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, enable_dueling_dqn=True):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.enable_dueling_dqn=enable_dueling_dqn\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "\n",
    "        if self.enable_dueling_dqn:\n",
    "            # Value stream\n",
    "            self.fc_value = nn.Linear(hidden_dim, 256)\n",
    "            self.value = nn.Linear(256, 1)\n",
    "\n",
    "            # Advantages stream\n",
    "            self.fc_advantages = nn.Linear(hidden_dim, 256)\n",
    "            self.advantages = nn.Linear(256, action_dim)\n",
    "\n",
    "        else:\n",
    "            self.output = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        if self.enable_dueling_dqn:\n",
    "            # Value calc\n",
    "            v = F.relu(self.fc_value(x))\n",
    "            V = self.value(v)\n",
    "\n",
    "            # Advantages calc\n",
    "            a = F.relu(self.fc_advantages(x))\n",
    "            A = self.advantages(a)\n",
    "\n",
    "            # Calc Q\n",
    "            Q = V + A - torch.mean(A, dim=1, keepdim=True)\n",
    "\n",
    "        else:\n",
    "            Q = self.output(x)\n",
    "\n",
    "        return Q\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     state_dim = 12\n",
    "#     action_dim = 2\n",
    "#     net = DQN(state_dim, action_dim)\n",
    "#     state = torch.randn(10, state_dim)\n",
    "#     output = net(state)\n",
    "#     print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define memory for Experience Replay\n",
    "from collections import deque\n",
    "import random\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen, seed=None):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "        # Optional seed for reproducibility\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import yaml\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "import itertools\n",
    "\n",
    "import flappy_bird_gymnasium\n",
    "import os\n",
    "\n",
    "# For printing date and time\n",
    "DATE_FORMAT = \"%m-%d %H:%M:%S\"\n",
    "\n",
    "# Directory for saving run info\n",
    "RUNS_DIR = \"runs\"\n",
    "os.makedirs(RUNS_DIR, exist_ok=True)\n",
    "\n",
    "# 'Agg': used to generate plots as images and save them to a file instead of rendering to screen\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu' # force cpu, sometimes GPU not always faster than CPU due to overhead of moving data to GPU\n",
    "\n",
    "# Deep Q-Learning Agent\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, hyperparameter_set):\n",
    "        with open('hyperparameters.yml', 'r') as file:\n",
    "            all_hyperparameter_sets = yaml.safe_load(file)\n",
    "            hyperparameters = all_hyperparameter_sets[hyperparameter_set]\n",
    "            # print(hyperparameters)\n",
    "\n",
    "        self.hyperparameter_set = hyperparameter_set\n",
    "\n",
    "        # Hyperparameters (adjustable)\n",
    "        self.env_id             = hyperparameters['env_id']\n",
    "        self.learning_rate_a    = hyperparameters['learning_rate_a']        # learning rate (alpha)\n",
    "        self.discount_factor_g  = hyperparameters['discount_factor_g']      # discount rate (gamma)\n",
    "        self.network_sync_rate  = hyperparameters['network_sync_rate']      # number of steps the agent takes before syncing the policy and target network\n",
    "        self.replay_memory_size = hyperparameters['replay_memory_size']     # size of replay memory\n",
    "        self.mini_batch_size    = hyperparameters['mini_batch_size']        # size of the training data set sampled from the replay memory\n",
    "        self.epsilon_init       = hyperparameters['epsilon_init']           # 1 = 100% random actions\n",
    "        self.epsilon_decay      = hyperparameters['epsilon_decay']          # epsilon decay rate\n",
    "        self.epsilon_min        = hyperparameters['epsilon_min']            # minimum epsilon value\n",
    "        self.stop_on_reward     = hyperparameters['stop_on_reward']         # stop training after reaching this number of rewards\n",
    "        self.fc1_nodes          = hyperparameters['fc1_nodes']\n",
    "        self.env_make_params    = hyperparameters.get('env_make_params',{}) # Get optional environment-specific parameters, default to empty dict\n",
    "        self.enable_double_dqn  = hyperparameters['enable_double_dqn']      # double dqn on/off flag\n",
    "        self.enable_dueling_dqn = hyperparameters['enable_dueling_dqn']     # dueling dqn on/off flag\n",
    "\n",
    "        # Neural Network\n",
    "        self.loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
    "        self.optimizer = None                # NN Optimizer. Initialize later.\n",
    "\n",
    "        # Path to Run info\n",
    "        self.LOG_FILE   = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.log')\n",
    "        self.MODEL_FILE = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.pt')\n",
    "        self.GRAPH_FILE = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.png')\n",
    "\n",
    "    def run(self, is_training=True, render=False):\n",
    "        if is_training:\n",
    "            start_time = datetime.now()\n",
    "            last_graph_update_time = start_time\n",
    "\n",
    "            log_message = f\"{start_time.strftime(DATE_FORMAT)}: Training starting...\"\n",
    "            print(log_message)\n",
    "            with open(self.LOG_FILE, 'w') as file:\n",
    "                file.write(log_message + '\\n')\n",
    "\n",
    "        # Create instance of the environment.\n",
    "        # Use \"**self.env_make_params\" to pass in environment-specific parameters from hyperparameters.yml.\n",
    "        env = gym.make(self.env_id, render_mode='human' if render else None, **self.env_make_params)\n",
    "\n",
    "        # Number of possible actions\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Get observation space size\n",
    "        num_states = env.observation_space.shape[0] # Expecting type: Box(low, high, (shape0,), float64)\n",
    "\n",
    "        # List to keep track of rewards collected per episode.\n",
    "        rewards_per_episode = []\n",
    "\n",
    "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
    "        policy_dqn = DQN(num_states, num_actions, self.fc1_nodes, self.enable_dueling_dqn).to(device)\n",
    "\n",
    "        if is_training:\n",
    "            # Initialize epsilon\n",
    "            epsilon = self.epsilon_init\n",
    "\n",
    "            # Initialize replay memory\n",
    "            memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "            # Create the target network and make it identical to the policy network\n",
    "            target_dqn = DQN(num_states, num_actions, self.fc1_nodes, self.enable_dueling_dqn).to(device)\n",
    "            target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "            # Policy network optimizer. \"Adam\" optimizer can be swapped to something else.\n",
    "            self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "\n",
    "            # List to keep track of epsilon decay\n",
    "            epsilon_history = []\n",
    "\n",
    "            # Track number of steps taken. Used for syncing policy => target network.\n",
    "            step_count=0\n",
    "\n",
    "            # Track best reward\n",
    "            best_reward = -9999999\n",
    "        else:\n",
    "            # Load learned policy\n",
    "            policy_dqn.load_state_dict(torch.load(self.MODEL_FILE))\n",
    "\n",
    "            # switch model to evaluation mode\n",
    "            policy_dqn.eval()\n",
    "\n",
    "        # Train INDEFINITELY, manually stop the run when you are satisfied (or unsatisfied) with the results\n",
    "        for episode in itertools.count():\n",
    "\n",
    "            state, _ = env.reset()  # Initialize environment. Reset returns (state,info).\n",
    "            state = torch.tensor(state, dtype=torch.float, device=device) # Convert state to tensor directly on device\n",
    "\n",
    "            terminated = False      # True when agent reaches goal or fails\n",
    "            episode_reward = 0.0    # Used to accumulate rewards per episode\n",
    "\n",
    "            # Perform actions until episode terminates or reaches max rewards\n",
    "            # (on some envs, it is possible for the agent to train to a point where it NEVER terminates, so stop on reward is necessary)\n",
    "            while(not terminated and episode_reward < self.stop_on_reward):\n",
    "\n",
    "                # Select action based on epsilon-greedy\n",
    "                if is_training and random.random() < epsilon:\n",
    "                    # select random action\n",
    "                    action = env.action_space.sample()\n",
    "                    action = torch.tensor(action, dtype=torch.int64, device=device)\n",
    "                else:\n",
    "                    # select best action\n",
    "                    with torch.no_grad():\n",
    "                        # state.unsqueeze(dim=0): Pytorch expects a batch layer, so add batch dimension i.e. tensor([1, 2, 3]) unsqueezes to tensor([[1, 2, 3]])\n",
    "                        # policy_dqn returns tensor([[1], [2], [3]]), so squeeze it to tensor([1, 2, 3]).\n",
    "                        # argmax finds the index of the largest element.\n",
    "                        action = policy_dqn(state.unsqueeze(dim=0)).squeeze().argmax()\n",
    "\n",
    "                # Execute action. Truncated and info is not used.\n",
    "                new_state,reward,terminated,truncated,info = env.step(action.item())\n",
    "\n",
    "                # Accumulate rewards\n",
    "                episode_reward += reward\n",
    "\n",
    "                # Convert new state and reward to tensors on device\n",
    "                new_state = torch.tensor(new_state, dtype=torch.float, device=device)\n",
    "                reward = torch.tensor(reward, dtype=torch.float, device=device)\n",
    "\n",
    "                if is_training:\n",
    "                    # Save experience into memory\n",
    "                    memory.append((state, action, new_state, reward, terminated))\n",
    "\n",
    "                    # Increment step counter\n",
    "                    step_count+=1\n",
    "\n",
    "                # Move to the next state\n",
    "                state = new_state\n",
    "\n",
    "            # Keep track of the rewards collected per episode.\n",
    "            rewards_per_episode.append(episode_reward)\n",
    "\n",
    "            # Save model when new best reward is obtained.\n",
    "            if is_training:\n",
    "                if episode_reward > best_reward:\n",
    "                    log_message = f\"{datetime.now().strftime(DATE_FORMAT)}: New best reward {episode_reward:0.1f} ({(episode_reward-best_reward)/best_reward*100:+.1f}%) at episode {episode}, saving model...\"\n",
    "                    print(log_message)\n",
    "                    with open(self.LOG_FILE, 'a') as file:\n",
    "                        file.write(log_message + '\\n')\n",
    "\n",
    "                    torch.save(policy_dqn.state_dict(), self.MODEL_FILE)\n",
    "                    best_reward = episode_reward\n",
    "\n",
    "\n",
    "                # Update graph every x seconds\n",
    "                current_time = datetime.now()\n",
    "                if current_time - last_graph_update_time > timedelta(seconds=10):\n",
    "                    self.save_graph(rewards_per_episode, epsilon_history)\n",
    "                    last_graph_update_time = current_time\n",
    "\n",
    "                # If enough experience has been collected\n",
    "                if len(memory)>self.mini_batch_size:\n",
    "                    mini_batch = memory.sample(self.mini_batch_size)\n",
    "                    self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                    # Decay epsilon\n",
    "                    epsilon = max(epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "                    epsilon_history.append(epsilon)\n",
    "\n",
    "                    # Copy policy network to target network after a certain number of steps\n",
    "                    if step_count > self.network_sync_rate:\n",
    "                        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                        step_count=0\n",
    "\n",
    "\n",
    "    def save_graph(self, rewards_per_episode, epsilon_history):\n",
    "        # Save plots\n",
    "        fig = plt.figure(1)\n",
    "\n",
    "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
    "        mean_rewards = np.zeros(len(rewards_per_episode))\n",
    "        for x in range(len(mean_rewards)):\n",
    "            mean_rewards[x] = np.mean(rewards_per_episode[max(0, x-99):(x+1)])\n",
    "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
    "        # plt.xlabel('Episodes')\n",
    "        plt.ylabel('Mean Rewards')\n",
    "        plt.plot(mean_rewards)\n",
    "\n",
    "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
    "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
    "        # plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Epsilon Decay')\n",
    "        plt.plot(epsilon_history)\n",
    "\n",
    "        plt.subplots_adjust(wspace=1.0, hspace=1.0)\n",
    "\n",
    "        # Save plots\n",
    "        fig.savefig(self.GRAPH_FILE)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    # Optimize policy network\n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "\n",
    "        # Transpose the list of experiences and separate each element\n",
    "        states, actions, new_states, rewards, terminations = zip(*mini_batch)\n",
    "\n",
    "        # Stack tensors to create batch tensors\n",
    "        # tensor([[1,2,3]])\n",
    "        states = torch.stack(states)\n",
    "\n",
    "        actions = torch.stack(actions)\n",
    "\n",
    "        new_states = torch.stack(new_states)\n",
    "\n",
    "        rewards = torch.stack(rewards)\n",
    "        terminations = torch.tensor(terminations).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.enable_double_dqn:\n",
    "                best_actions_from_policy = policy_dqn(new_states).argmax(dim=1)\n",
    "\n",
    "                target_q = rewards + (1-terminations) * self.discount_factor_g * \\\n",
    "                                target_dqn(new_states).gather(dim=1, index=best_actions_from_policy.unsqueeze(dim=1)).squeeze()\n",
    "            else:\n",
    "                # Calculate target Q values (expected returns)\n",
    "                target_q = rewards + (1-terminations) * self.discount_factor_g * target_dqn(new_states).max(dim=1)[0]\n",
    "                '''\n",
    "                    target_dqn(new_states)  ==> tensor([[1,2,3],[4,5,6]])\n",
    "                        .max(dim=1)         ==> torch.return_types.max(values=tensor([3,6]), indices=tensor([3, 0, 0, 1]))\n",
    "                            [0]             ==> tensor([3,6])\n",
    "                '''\n",
    "\n",
    "        # Calcuate Q values from current policy\n",
    "        current_q = policy_dqn(states).gather(dim=1, index=actions.unsqueeze(dim=1)).squeeze()\n",
    "        '''\n",
    "            policy_dqn(states)  ==> tensor([[1,2,3],[4,5,6]])\n",
    "                actions.unsqueeze(dim=1)\n",
    "                .gather(1, actions.unsqueeze(dim=1))  ==>\n",
    "                    .squeeze()                    ==>\n",
    "        '''\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(current_q, target_q)\n",
    "\n",
    "        # Optimize the model (backpropagation)\n",
    "        self.optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()             # Compute gradients\n",
    "        self.optimizer.step()       # Update network parameters i.e. weights and biases\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # Parse command line inputs\n",
    "#     parser = argparse.ArgumentParser(description='Train or test model.')\n",
    "#     parser.add_argument('hyperparameters', help='')\n",
    "#     parser.add_argument('--train', help='Training mode', action='store_true')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     dql = Agent(hyperparameter_set=args.hyperparameters)\n",
    "\n",
    "#     if args.train:\n",
    "#         dql.run(is_training=True)\n",
    "#     else:\n",
    "#         dql.run(is_training=False, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-20 12:41:42: Training starting...\n",
      "01-20 12:41:43: New best reward 11.0 (-100.0%) at episode 0, saving model...\n",
      "01-20 12:41:43: New best reward 13.0 (+18.2%) at episode 1, saving model...\n",
      "01-20 12:41:43: New best reward 18.0 (+38.5%) at episode 2, saving model...\n",
      "01-20 12:41:43: New best reward 19.0 (+5.6%) at episode 3, saving model...\n",
      "01-20 12:41:43: New best reward 21.0 (+10.5%) at episode 9, saving model...\n",
      "01-20 12:41:43: New best reward 27.0 (+28.6%) at episode 10, saving model...\n",
      "01-20 12:41:43: New best reward 35.0 (+29.6%) at episode 13, saving model...\n",
      "01-20 12:41:43: New best reward 45.0 (+28.6%) at episode 25, saving model...\n",
      "01-20 12:41:43: New best reward 46.0 (+2.2%) at episode 27, saving model...\n",
      "01-20 12:41:43: New best reward 50.0 (+8.7%) at episode 30, saving model...\n",
      "01-20 12:41:43: New best reward 66.0 (+32.0%) at episode 39, saving model...\n",
      "01-20 12:41:43: New best reward 68.0 (+3.0%) at episode 45, saving model...\n",
      "01-20 12:41:43: New best reward 71.0 (+4.4%) at episode 134, saving model...\n",
      "01-20 12:41:44: New best reward 77.0 (+8.5%) at episode 275, saving model...\n",
      "01-20 12:41:44: New best reward 90.0 (+16.9%) at episode 359, saving model...\n",
      "01-20 12:41:44: New best reward 111.0 (+23.3%) at episode 387, saving model...\n",
      "01-20 12:41:44: New best reward 139.0 (+25.2%) at episode 567, saving model...\n",
      "01-20 12:41:45: New best reward 208.0 (+49.6%) at episode 691, saving model...\n",
      "01-20 12:41:45: New best reward 226.0 (+8.7%) at episode 905, saving model...\n",
      "01-20 12:41:46: New best reward 233.0 (+3.1%) at episode 1168, saving model...\n",
      "01-20 12:41:47: New best reward 242.0 (+3.9%) at episode 1261, saving model...\n",
      "01-20 12:41:47: New best reward 251.0 (+3.7%) at episode 1266, saving model...\n",
      "01-20 12:41:47: New best reward 269.0 (+7.2%) at episode 1283, saving model...\n",
      "01-20 12:41:47: New best reward 273.0 (+1.5%) at episode 1305, saving model...\n",
      "01-20 12:41:48: New best reward 323.0 (+18.3%) at episode 1351, saving model...\n",
      "01-20 12:41:48: New best reward 338.0 (+4.6%) at episode 1361, saving model...\n",
      "01-20 12:41:48: New best reward 471.0 (+39.3%) at episode 1452, saving model...\n",
      "01-20 12:41:49: New best reward 511.0 (+8.5%) at episode 1500, saving model...\n",
      "01-20 12:41:49: New best reward 538.0 (+5.3%) at episode 1504, saving model...\n",
      "01-20 12:41:59: New best reward 582.0 (+8.2%) at episode 2499, saving model...\n",
      "01-20 12:42:01: New best reward 641.0 (+10.1%) at episode 2706, saving model...\n",
      "01-20 12:42:05: New best reward 644.0 (+0.5%) at episode 2962, saving model...\n",
      "01-20 12:43:45: New best reward 100000.0 (+15428.0%) at episode 9504, saving model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcartpole1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m agent\u001b[38;5;241m.\u001b[39msave_graph(agent\u001b[38;5;241m.\u001b[39mreward_episode, agent\u001b[38;5;241m.\u001b[39mepsilon_history)\n",
      "Cell \u001b[0;32mIn[5], line 148\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, is_training, render)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# select best action\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# state.unsqueeze(dim=0): Pytorch expects a batch layer, so add batch dimension i.e. tensor([1, 2, 3]) unsqueezes to tensor([[1, 2, 3]])\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;66;03m# policy_dqn returns tensor([[1], [2], [3]]), so squeeze it to tensor([1, 2, 3]).\u001b[39;00m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;66;03m# argmax finds the index of the largest element.\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Execute action. Truncated and info is not used.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m new_state,reward,terminated,truncated,info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent('cartpole1')\n",
    "agent.run(is_training=True,render=False)\n",
    "\n",
    "agent.save_graph(agent.reward_episode, agent.epsilon_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kz/36nj487d0sgbg74v1kmzvdmc0000gn/T/ipykernel_90519/3501168421.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_dqn.load_state_dict(torch.load(self.MODEL_FILE))\n",
      "2025-01-20 12:45:39.782 python[90519:25374366] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-20 12:45:39.782 python[90519:25374366] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 151\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, is_training, render)\u001b[0m\n\u001b[1;32m    148\u001b[0m         action \u001b[38;5;241m=\u001b[39m policy_dqn(state\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39margmax()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Execute action. Truncated and info is not used.\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m new_state,reward,terminated,truncated,info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Accumulate rewards\u001b[39;00m\n\u001b[1;32m    154\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[0;32m~/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:223\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    220\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sutton_barto_reward \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/Documents/data-analysis/advanced/Lab9/notebooks/netflixApp/.conda/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:338\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 338\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    342\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    343\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.run(is_training=False,render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-20 12:47:00: Training starting...\n",
      "01-20 12:47:00: New best reward -7.5 (-100.0%) at episode 0, saving model...\n",
      "01-20 12:47:00: New best reward -4.5 (-40.0%) at episode 2, saving model...\n",
      "01-20 12:47:01: New best reward -3.9 (-13.3%) at episode 263, saving model...\n",
      "01-20 12:47:02: New best reward -2.7 (-30.8%) at episode 590, saving model...\n",
      "01-20 12:47:02: New best reward -1.5 (-44.4%) at episode 640, saving model...\n",
      "01-20 12:47:02: New best reward 0.3 (-120.0%) at episode 657, saving model...\n",
      "01-20 12:47:05: New best reward 0.9 (+200.0%) at episode 1739, saving model...\n",
      "01-20 12:47:08: New best reward 3.3 (+266.7%) at episode 2687, saving model...\n",
      "01-20 12:47:10: New best reward 3.9 (+18.2%) at episode 3374, saving model...\n",
      "01-20 12:47:17: New best reward 4.1 (+5.1%) at episode 5370, saving model...\n",
      "01-20 12:47:36: New best reward 6.2 (+51.2%) at episode 10817, saving model...\n",
      "01-20 12:47:45: New best reward 6.6 (+6.5%) at episode 13079, saving model...\n",
      "01-20 12:47:46: New best reward 8.4 (+27.3%) at episode 13268, saving model...\n",
      "01-20 12:49:07: New best reward 10.6 (+26.2%) at episode 30457, saving model...\n",
      "01-20 12:49:18: New best reward 10.7 (+0.9%) at episode 32678, saving model...\n",
      "01-20 12:49:26: New best reward 10.8 (+0.9%) at episode 34128, saving model...\n",
      "01-20 12:51:02: New best reward 12.9 (+19.4%) at episode 51599, saving model...\n",
      "01-20 12:51:17: New best reward 13.7 (+6.2%) at episode 54091, saving model...\n",
      "01-20 12:52:09: New best reward 15.6 (+13.9%) at episode 62388, saving model...\n",
      "01-20 12:52:25: New best reward 20.1 (+28.8%) at episode 64771, saving model...\n",
      "01-20 12:52:45: New best reward 26.9 (+33.8%) at episode 68019, saving model...\n",
      "01-20 12:53:23: New best reward 32.7 (+21.6%) at episode 73782, saving model...\n",
      "01-20 12:53:34: New best reward 34.6 (+5.8%) at episode 75477, saving model...\n",
      "01-20 12:54:09: New best reward 41.5 (+19.9%) at episode 80513, saving model...\n",
      "01-20 12:55:37: New best reward 46.7 (+12.5%) at episode 92418, saving model...\n",
      "01-20 12:56:48: New best reward 51.4 (+10.1%) at episode 101657, saving model...\n"
     ]
    }
   ],
   "source": [
    "F_agent = Agent('flappybird1')\n",
    "F_agent.run(is_training=True,render=False)\n",
    "F_agent.save_graph(F_agent.reward_episode, F_agent.epsilon_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
